# Local LLM for Home Assistant

Connect your Home Assistant to external LLM servers on your network (or internet) using a lightweight proxy add-on and custom integration.

## ğŸ¯ Architecture

```
Home Assistant â†’ Proxy Add-on â†’ Your LLM Server (Ollama/llama.cpp/etc.)
          â†“           â†“
    Integration   Port 8080
```

This repository provides:
1. **Proxy Add-on**: Lightweight gateway to your external LLM server
2. **Custom Integration**: Connects to the proxy to provide conversation capabilities

## ğŸš€ Quick Start

### Step 1: Install the Proxy Add-on

1. In Home Assistant, go to **Settings** â†’ **Add-ons** â†’ **Add-on Store**
2. Click the three dots (â‹®) â†’ **Repositories**
3. Add: `https://github.com/tylergraf/local_llm`
4. Find "Local LLM Proxy" and click **Install**
5. Configure with your LLM server details:
   ```json
   {
     "llm_server_url": "http://192.168.1.100:11434",
     "api_key": "",
     "timeout": 120
   }
   ```
6. Click **Start**

### Step 2: Install the Integration

**Via HACS** (recommended):
1. Go to **HACS** â†’ **Integrations**  
2. Click â‹® â†’ **Custom repositories**
3. Add: `https://github.com/tylergraf/local_llm`
4. Category: **Integration**
5. Download and install
6. Restart Home Assistant

**Manual**:
1. Copy `custom_components/local_llm` to your HA config directory
2. Restart Home Assistant

### Step 3: Configure the Integration

1. Go to **Settings** â†’ **Devices & Services**
2. Click **+ Add Integration**
3. Search for "Local LLM"
4. Configure:
   - **Base URL**: `http://localhost:8080/v1`
   - **API Key**: (leave blank)
   - **Model**: Your model name (e.g., `llama3.2:3b`)

## âœ¨ Features

### Proxy Add-on
- ğŸª¶ **Lightweight**: Minimal resource usage (~30-50 MB RAM)
- ğŸ›ï¸ **Easy Config**: Configure through Home Assistant UI
- ğŸ”„ **Hot Reload**: Change LLM servers without reconfiguring integration
- ğŸ›¡ï¸ **Error Handling**: Better timeout and error management
- ğŸŒ **Network Gateway**: Handles all external LLM communication

### Custom Integration
- ğŸ’¬ **Full Conversation Agent**: Complete Home Assistant conversation support
- ğŸ¨ **Service Calls**: Use in automations and scripts
- ğŸ”§ **Configurable**: Custom prompts and settings
- ğŸ“Š **Status Monitoring**: See connection health

## ğŸ§  Supported LLM Servers

Works with any OpenAI-compatible API:

- **[Ollama](https://ollama.com/)** - Easy local LLM server
- **[llama.cpp](https://github.com/ggerganov/llama.cpp)** - Lightweight C++ implementation
- **[LM Studio](https://lmstudio.ai/)** - Desktop LLM server with GUI
- **[vLLM](https://github.com/vllm-project/vllm)** - High-performance serving
- **[LocalAI](https://localai.io/)** - Self-hosted OpenAI alternative
- **[text-generation-webui](https://github.com/oobabooga/text-generation-webui)** - Feature-rich web UI
- Any other OpenAI-compatible server

## ğŸ’¡ Why Use the Proxy Add-on?

### Option A: With Proxy Add-on (Recommended)
```
HA Integration â†’ Proxy Add-on â†’ External LLM Server
```
âœ… Easy configuration through UI  
âœ… Centralized management  
âœ… Change servers without reconfiguring  
âœ… Better error handling  

### Option B: Direct Connection
```
HA Integration â†’ External LLM Server
```
âŒ Must reconfigure integration to change servers  
âŒ Docker networking complexity  
âŒ Less flexible  

## ğŸ“‹ Example Configurations

### Ollama on Your Network
```json
{
  "llm_server_url": "http://192.168.1.50:11434",
  "api_key": "",
  "timeout": 120
}
```

### LM Studio on Another Machine
```json
{
  "llm_server_url": "http://10.0.0.100:1234",
  "api_key": "",
  "timeout": 120
}
```

### Remote Server with Authentication
```json
{
  "llm_server_url": "https://my-llm.example.com",
  "api_key": "your-api-key",
  "timeout": 180
}
```

## ğŸ”§ Setting Up Your LLM Server

### Ollama (Easiest)

**Mac/Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2:3b
ollama serve
```

**Windows:**  
Download from [ollama.com](https://ollama.com/)

### LM Studio

1. Download [LM Studio](https://lmstudio.ai/)
2. Download a model
3. Load model and start local server
4. Note the port (usually 1234)

### llama.cpp

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
./server -m models/your-model.gguf --host 0.0.0.0 --port 8080
```

## ğŸ› Troubleshooting

### Add-on won't start
- Check add-on logs for specific errors
- Verify LLM server URL is correct and reachable
- Test connection: `curl http://your-server:11434/v1/models`

### Integration can't connect to proxy
- Ensure proxy add-on is running
- Check add-on logs
- Verify integration URL is `http://localhost:8080/v1`

### Timeout errors
- Increase timeout in add-on config
- Check if LLM server is overloaded
- Try a smaller/faster model

### Home Assistant in Docker can't reach LAN LLM
The proxy add-on solves this! It handles the networking.

But if needed:
- In add-on config, use your machine's IP instead of localhost
- Example: `http://192.168.1.50:11434` instead of `http://localhost:11434`

## ğŸ“ Repository Structure

```
local_llm/
â”œâ”€â”€ repository.json              # Add-on repository metadata
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ INSTALL.md                   # Detailed installation guide
â”œâ”€â”€ custom_components/           # Custom integration
â”‚   â””â”€â”€ local_llm/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ conversation.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ local-llm-proxy/            # Proxy add-on
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ proxy_server.py         # Lightweight Python proxy
â”‚   â”œâ”€â”€ run.sh
â”‚   â””â”€â”€ ...
â””â”€â”€ hacs.json                   # HACS metadata
```

## ğŸ¤ Contributing

Contributions welcome! Please submit issues and pull requests.

## ğŸ“„ License

MIT License

## ğŸ”— Links

- [Home Assistant](https://www.home-assistant.io/)
- [Ollama](https://ollama.com/)
- [HACS](https://hacs.xyz/)
- [GitHub Issues](https://github.com/tylergraf/local_llm/issues)

